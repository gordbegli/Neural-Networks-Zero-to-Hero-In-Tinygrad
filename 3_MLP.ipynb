{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d38cc3-155c-4909-b3f1-3d1e43523c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-24.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tinygrad in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tinygrad) (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip && pip install numpy && pip install tinygrad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4320baf-ea82-44a4-83f8-0990038377ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tinygrad.helpers import Timing\n",
    "from tinygrad import Tensor\n",
    "from tinygrad import dtypes\n",
    "from tinygrad.nn.optim import SGD\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "def sparse_categorical_crossentropy(self, Y, ignore_index=-1) -> Tensor:\n",
    "    loss_mask = Y != ignore_index\n",
    "    y_counter = Tensor.arange(self.shape[-1], dtype=dtypes.int32, requires_grad=False, device=self.device).unsqueeze(0).expand(Y.numel(), self.shape[-1])\n",
    "    y = ((y_counter == Y.flatten().reshape(-1, 1)).where(-1.0, 0) * loss_mask.reshape(-1, 1)).reshape(*Y.shape, self.shape[-1])\n",
    "    return self.log_softmax().mul(y).sum() / loss_mask.sum()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "class Linear:\n",
    "  def __init__(self, in_features, out_features, bias=True, initialization: str='kaiming_uniform'):\n",
    "    self.weight = getattr(Tensor, initialization)(in_features, out_features)\n",
    "    self.bias = Tensor.zeros(out_features) if bias else None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return x.linear(self.weight.transpose(), self.bias)\n",
    "\n",
    "class TinyNet:\n",
    "  def __init__(self):\n",
    "    self.C = Linear(27, 10, bias=False)\n",
    "    self.W1 = Linear(30, 200, bias=True)\n",
    "    self.W2 = Linear(200, 27, bias=True)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    emb = self.C.weight[x]\n",
    "    h = Tensor.tanh(emb.view(-1, 30) @ self.W1.weight + self.W1.bias)\n",
    "    logits = h @ self.W2.weight + self.W2.bias\n",
    "    loss = sparse_categorical_crossentropy(logits, Ytr[ix])\n",
    "    return loss\n",
    "\n",
    "net = TinyNet()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "words = open('./sandbox/names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    ix = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    context = context[1:] + [ix] \n",
    "  \n",
    "X = Tensor(X)\n",
    "Y = Tensor(Y)\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] \n",
    "\n",
    "  X = Tensor(X)\n",
    "  Y = Tensor(Y)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.1\n",
    "opt = SGD([net.C.weight, net.W1.weight, net.W1.bias, net.W2.weight, net.W2.bias], lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8b53069-ff84-4d05-9dd9-e5b8a02425b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1127124\n"
     ]
    }
   ],
   "source": [
    "with Tensor.train():\n",
    "    for step in range(100):\n",
    "        #random sample a batch / Minibatch construct \n",
    "        ix = Tensor.randint(32, low=0, high=Xtr.shape[0])\n",
    "\n",
    "        #forward pass / Forward pass\n",
    "        loss = net(Xtr[ix])\n",
    "        \n",
    "        #zero gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        #backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        #update parameters\n",
    "        opt.step()\n",
    "        \n",
    "        if step > 100:\n",
    "            learning_rate *= 0.1\n",
    "\n",
    "print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82970fb3-76c7-4045-a8cc-cc34198dd467",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m   emb \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mC\u001b[38;5;241m.\u001b[39mweight[Tensor([context])] \u001b[38;5;66;03m# (1,block_size,d)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m   h \u001b[38;5;241m=\u001b[39m Tensor\u001b[38;5;241m.\u001b[39mtanh(emb\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m \u001b[43mW1\u001b[49m \u001b[38;5;241m+\u001b[39m b1)\n\u001b[1;32m      8\u001b[0m   logits \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m@\u001b[39m W2 \u001b[38;5;241m+\u001b[39m b2\n\u001b[1;32m      9\u001b[0m   probs \u001b[38;5;241m=\u001b[39m Tensor\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'W1' is not defined"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      emb = net.C.weight[Tensor([context])] # (1,block_size,d)\n",
    "      h = Tensor.tanh(emb.view(1, -1) @ net.W1 + net.b1)\n",
    "      logits = h @ W2 + b2\n",
    "      probs = Tensor.softmax(logits, dim=1)\n",
    "      ix = Tensor.multinomial(probs, num_samples=1).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15f6b535-6aa5-4b82-9579-b1f6aace45bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Loss: 3.8755383491516113 | Learning Rate: 0.1\n",
      "Step 101 | Loss: 2.3160030841827393 | Learning Rate: 0.1\n",
      "Step 201 | Loss: 2.535747766494751 | Learning Rate: 0.1\n",
      "Step 301 | Loss: 2.587575912475586 | Learning Rate: 0.1\n",
      "Step 401 | Loss: 2.070578098297119 | Learning Rate: 0.1\n",
      "Step 501 | Loss: 2.48221755027771 | Learning Rate: 0.1\n",
      "Step 601 | Loss: 2.5466513633728027 | Learning Rate: 0.1\n",
      "Step 701 | Loss: 2.385951280593872 | Learning Rate: 0.1\n",
      "Step 801 | Loss: 2.440654754638672 | Learning Rate: 0.1\n",
      "Step 901 | Loss: 2.6263201236724854 | Learning Rate: 0.1\n",
      "Step 1001 | Loss: 2.4832701683044434 | Learning Rate: 0.1\n",
      "Step 1101 | Loss: 2.119320869445801 | Learning Rate: 0.010000000000000002\n",
      "Step 1201 | Loss: 2.4285807609558105 | Learning Rate: 0.010000000000000002\n",
      "Step 1301 | Loss: 2.407773494720459 | Learning Rate: 0.010000000000000002\n",
      "Step 1401 | Loss: 2.5390570163726807 | Learning Rate: 0.010000000000000002\n",
      "Step 1501 | Loss: 2.300238847732544 | Learning Rate: 0.010000000000000002\n",
      "Step 1601 | Loss: 2.6651625633239746 | Learning Rate: 0.010000000000000002\n",
      "Step 1701 | Loss: 2.1783485412597656 | Learning Rate: 0.010000000000000002\n",
      "Step 1801 | Loss: 2.3893444538116455 | Learning Rate: 0.010000000000000002\n",
      "Step 1901 | Loss: 2.503650426864624 | Learning Rate: 0.010000000000000002\n",
      "Step 2001 | Loss: 2.663987636566162 | Learning Rate: 0.010000000000000002\n",
      "Step 2101 | Loss: 2.1737589836120605 | Learning Rate: 0.0010000000000000002\n",
      "Step 2201 | Loss: 2.149174690246582 | Learning Rate: 0.0010000000000000002\n",
      "Step 2301 | Loss: 2.492805004119873 | Learning Rate: 0.0010000000000000002\n",
      "Step 2401 | Loss: 2.060487985610962 | Learning Rate: 0.0010000000000000002\n",
      "Step 2501 | Loss: 2.7120630741119385 | Learning Rate: 0.0010000000000000002\n",
      "Step 2601 | Loss: 2.637115478515625 | Learning Rate: 0.0010000000000000002\n",
      "Step 2701 | Loss: 2.0879099369049072 | Learning Rate: 0.0010000000000000002\n",
      "Step 2801 | Loss: 2.5045979022979736 | Learning Rate: 0.0010000000000000002\n",
      "Step 2901 | Loss: 2.282252788543701 | Learning Rate: 0.0010000000000000002\n",
      "Step 3001 | Loss: 2.87717866897583 | Learning Rate: 0.0010000000000000002\n",
      "Step 3101 | Loss: 2.4452781677246094 | Learning Rate: 0.00010000000000000003\n",
      "Step 3201 | Loss: 2.450775146484375 | Learning Rate: 0.00010000000000000003\n",
      "Step 3301 | Loss: 2.3566014766693115 | Learning Rate: 0.00010000000000000003\n",
      "Step 3401 | Loss: 2.4726107120513916 | Learning Rate: 0.00010000000000000003\n",
      "Step 3501 | Loss: 2.228271484375 | Learning Rate: 0.00010000000000000003\n",
      "Step 3601 | Loss: 2.8612239360809326 | Learning Rate: 0.00010000000000000003\n",
      "Step 3701 | Loss: 2.4584693908691406 | Learning Rate: 0.00010000000000000003\n",
      "Step 3801 | Loss: 2.029449939727783 | Learning Rate: 0.00010000000000000003\n",
      "Step 3901 | Loss: 2.2858378887176514 | Learning Rate: 0.00010000000000000003\n",
      "Step 4001 | Loss: 2.2953310012817383 | Learning Rate: 0.00010000000000000003\n",
      "Step 4101 | Loss: 1.9939584732055664 | Learning Rate: 1.0000000000000004e-05\n",
      "Step 4201 | Loss: 2.3101611137390137 | Learning Rate: 1.0000000000000004e-05\n",
      "Step 4301 | Loss: 2.6920435428619385 | Learning Rate: 1.0000000000000004e-05\n",
      "Step 4401 | Loss: 2.542203187942505 | Learning Rate: 1.0000000000000004e-05\n",
      "Step 4501 | Loss: 2.5058350563049316 | Learning Rate: 1.0000000000000004e-05\n",
      "Step 4601 | Loss: 2.2922780513763428 | Learning Rate: 1.0000000000000004e-05\n",
      "Step 4701 | Loss: 2.559873104095459 | Learning Rate: 1.0000000000000004e-05\n",
      "Step 4801 | Loss: 2.3499367237091064 | Learning Rate: 1.0000000000000004e-05\n",
      "Step 4901 | Loss: 2.4131407737731934 | Learning Rate: 1.0000000000000004e-05\n",
      "Step 5001 | Loss: 2.22674298286438 | Learning Rate: 1.0000000000000004e-05\n",
      "Step 5101 | Loss: 2.61398983001709 | Learning Rate: 1.0000000000000004e-06\n",
      "Step 5201 | Loss: 2.27944016456604 | Learning Rate: 1.0000000000000004e-06\n",
      "Step 5301 | Loss: 2.0326900482177734 | Learning Rate: 1.0000000000000004e-06\n",
      "Step 5401 | Loss: 2.368366241455078 | Learning Rate: 1.0000000000000004e-06\n",
      "Step 5501 | Loss: 2.5475363731384277 | Learning Rate: 1.0000000000000004e-06\n",
      "Step 5601 | Loss: 2.104217529296875 | Learning Rate: 1.0000000000000004e-06\n",
      "Step 5701 | Loss: 2.564643621444702 | Learning Rate: 1.0000000000000004e-06\n",
      "Step 5801 | Loss: 2.099804401397705 | Learning Rate: 1.0000000000000004e-06\n",
      "Step 5901 | Loss: 2.4129977226257324 | Learning Rate: 1.0000000000000004e-06\n",
      "Step 6001 | Loss: 2.56636118888855 | Learning Rate: 1.0000000000000004e-06\n",
      "Step 6101 | Loss: 2.534069776535034 | Learning Rate: 1.0000000000000005e-07\n",
      "Step 6201 | Loss: 2.607773780822754 | Learning Rate: 1.0000000000000005e-07\n",
      "Step 6301 | Loss: 2.1210341453552246 | Learning Rate: 1.0000000000000005e-07\n",
      "Step 6401 | Loss: 2.490509033203125 | Learning Rate: 1.0000000000000005e-07\n",
      "Step 6501 | Loss: 2.1771161556243896 | Learning Rate: 1.0000000000000005e-07\n",
      "Step 6601 | Loss: 2.582977294921875 | Learning Rate: 1.0000000000000005e-07\n",
      "Step 6701 | Loss: 2.310152053833008 | Learning Rate: 1.0000000000000005e-07\n",
      "Step 6801 | Loss: 2.0631046295166016 | Learning Rate: 1.0000000000000005e-07\n",
      "Step 6901 | Loss: 2.366650104522705 | Learning Rate: 1.0000000000000005e-07\n",
      "Step 7001 | Loss: 2.4243712425231934 | Learning Rate: 1.0000000000000005e-07\n",
      "Step 7101 | Loss: 2.6986708641052246 | Learning Rate: 1.0000000000000005e-08\n",
      "Step 7201 | Loss: 2.542459487915039 | Learning Rate: 1.0000000000000005e-08\n",
      "Step 7301 | Loss: 2.4983835220336914 | Learning Rate: 1.0000000000000005e-08\n",
      "Step 7401 | Loss: 2.565227746963501 | Learning Rate: 1.0000000000000005e-08\n",
      "Step 7501 | Loss: 2.3037664890289307 | Learning Rate: 1.0000000000000005e-08\n",
      "Step 7601 | Loss: 2.1532187461853027 | Learning Rate: 1.0000000000000005e-08\n",
      "Step 7701 | Loss: 2.4936444759368896 | Learning Rate: 1.0000000000000005e-08\n",
      "Step 7801 | Loss: 2.2526464462280273 | Learning Rate: 1.0000000000000005e-08\n",
      "Step 7901 | Loss: 2.373807191848755 | Learning Rate: 1.0000000000000005e-08\n",
      "Step 8001 | Loss: 2.2970798015594482 | Learning Rate: 1.0000000000000005e-08\n",
      "Step 8101 | Loss: 2.268179178237915 | Learning Rate: 1.0000000000000005e-09\n",
      "Step 8201 | Loss: 2.576104164123535 | Learning Rate: 1.0000000000000005e-09\n",
      "Step 8301 | Loss: 2.482048749923706 | Learning Rate: 1.0000000000000005e-09\n",
      "Step 8401 | Loss: 2.188629388809204 | Learning Rate: 1.0000000000000005e-09\n",
      "Step 8501 | Loss: 2.295167922973633 | Learning Rate: 1.0000000000000005e-09\n",
      "Step 8601 | Loss: 2.738912582397461 | Learning Rate: 1.0000000000000005e-09\n",
      "Step 8701 | Loss: 2.312558889389038 | Learning Rate: 1.0000000000000005e-09\n",
      "Step 8801 | Loss: 2.653290271759033 | Learning Rate: 1.0000000000000005e-09\n",
      "Step 8901 | Loss: 2.0790159702301025 | Learning Rate: 1.0000000000000005e-09\n",
      "Step 9001 | Loss: 2.418311834335327 | Learning Rate: 1.0000000000000005e-09\n",
      "Step 9101 | Loss: 2.2870211601257324 | Learning Rate: 1.0000000000000006e-10\n",
      "Step 9201 | Loss: 2.382019281387329 | Learning Rate: 1.0000000000000006e-10\n",
      "Step 9301 | Loss: 2.3573708534240723 | Learning Rate: 1.0000000000000006e-10\n",
      "Step 9401 | Loss: 2.0487961769104004 | Learning Rate: 1.0000000000000006e-10\n",
      "Step 9501 | Loss: 2.3818917274475098 | Learning Rate: 1.0000000000000006e-10\n",
      "Step 9601 | Loss: 2.50799560546875 | Learning Rate: 1.0000000000000006e-10\n",
      "Step 9701 | Loss: 1.9626022577285767 | Learning Rate: 1.0000000000000006e-10\n",
      "Step 9801 | Loss: 2.1193971633911133 | Learning Rate: 1.0000000000000006e-10\n",
      "Step 9901 | Loss: 2.569425106048584 | Learning Rate: 1.0000000000000006e-10\n",
      "Step 10001 | Loss: 2.5630075931549072 | Learning Rate: 1.0000000000000006e-10\n",
      "Step 10101 | Loss: 2.578967809677124 | Learning Rate: 1.0000000000000006e-11\n",
      "Step 10201 | Loss: 2.5771853923797607 | Learning Rate: 1.0000000000000006e-11\n",
      "Step 10301 | Loss: 2.2779195308685303 | Learning Rate: 1.0000000000000006e-11\n",
      "Step 10401 | Loss: 2.4520163536071777 | Learning Rate: 1.0000000000000006e-11\n",
      "Step 10501 | Loss: 2.386209011077881 | Learning Rate: 1.0000000000000006e-11\n",
      "Step 10601 | Loss: 2.0964739322662354 | Learning Rate: 1.0000000000000006e-11\n",
      "Step 10701 | Loss: 2.0274970531463623 | Learning Rate: 1.0000000000000006e-11\n",
      "Step 10801 | Loss: 2.4680707454681396 | Learning Rate: 1.0000000000000006e-11\n",
      "Step 10901 | Loss: 2.4236631393432617 | Learning Rate: 1.0000000000000006e-11\n",
      "Step 11001 | Loss: 2.4287703037261963 | Learning Rate: 1.0000000000000006e-11\n",
      "Step 11101 | Loss: 2.8072259426116943 | Learning Rate: 1.0000000000000006e-12\n",
      "Step 11201 | Loss: 2.3505077362060547 | Learning Rate: 1.0000000000000006e-12\n",
      "Step 11301 | Loss: 2.332366704940796 | Learning Rate: 1.0000000000000006e-12\n",
      "Step 11401 | Loss: 2.34303879737854 | Learning Rate: 1.0000000000000006e-12\n",
      "Step 11501 | Loss: 2.152683734893799 | Learning Rate: 1.0000000000000006e-12\n",
      "Step 11601 | Loss: 2.3597023487091064 | Learning Rate: 1.0000000000000006e-12\n",
      "Step 11701 | Loss: 2.3426196575164795 | Learning Rate: 1.0000000000000006e-12\n",
      "Step 11801 | Loss: 2.4765546321868896 | Learning Rate: 1.0000000000000006e-12\n",
      "Step 11901 | Loss: 2.6456234455108643 | Learning Rate: 1.0000000000000006e-12\n",
      "Step 12001 | Loss: 2.827211856842041 | Learning Rate: 1.0000000000000006e-12\n",
      "Step 12101 | Loss: 2.204946279525757 | Learning Rate: 1.0000000000000007e-13\n",
      "Step 12201 | Loss: 2.4466965198516846 | Learning Rate: 1.0000000000000007e-13\n",
      "Step 12301 | Loss: 2.1835665702819824 | Learning Rate: 1.0000000000000007e-13\n",
      "Step 12401 | Loss: 2.5834579467773438 | Learning Rate: 1.0000000000000007e-13\n",
      "Step 12501 | Loss: 2.155233383178711 | Learning Rate: 1.0000000000000007e-13\n",
      "Step 12601 | Loss: 2.3642706871032715 | Learning Rate: 1.0000000000000007e-13\n",
      "Step 12701 | Loss: 2.6719417572021484 | Learning Rate: 1.0000000000000007e-13\n",
      "Step 12801 | Loss: 2.3771729469299316 | Learning Rate: 1.0000000000000007e-13\n",
      "Step 12901 | Loss: 2.342980146408081 | Learning Rate: 1.0000000000000007e-13\n",
      "Step 13001 | Loss: 2.1569669246673584 | Learning Rate: 1.0000000000000007e-13\n",
      "Step 13101 | Loss: 2.538996696472168 | Learning Rate: 1.0000000000000008e-14\n",
      "Step 13201 | Loss: 2.6558005809783936 | Learning Rate: 1.0000000000000008e-14\n",
      "Step 13301 | Loss: 2.285210132598877 | Learning Rate: 1.0000000000000008e-14\n",
      "Step 13401 | Loss: 2.3623321056365967 | Learning Rate: 1.0000000000000008e-14\n",
      "Step 13501 | Loss: 2.3799631595611572 | Learning Rate: 1.0000000000000008e-14\n",
      "Step 13601 | Loss: 2.0823237895965576 | Learning Rate: 1.0000000000000008e-14\n",
      "Step 13701 | Loss: 2.2574682235717773 | Learning Rate: 1.0000000000000008e-14\n",
      "Step 13801 | Loss: 2.1023190021514893 | Learning Rate: 1.0000000000000008e-14\n",
      "Step 13901 | Loss: 2.6669600009918213 | Learning Rate: 1.0000000000000008e-14\n",
      "Step 14001 | Loss: 2.407655715942383 | Learning Rate: 1.0000000000000008e-14\n",
      "Step 14101 | Loss: 2.231250047683716 | Learning Rate: 1.0000000000000009e-15\n",
      "Step 14201 | Loss: 2.26349139213562 | Learning Rate: 1.0000000000000009e-15\n",
      "Step 14301 | Loss: 2.2927181720733643 | Learning Rate: 1.0000000000000009e-15\n",
      "Step 14401 | Loss: 2.176647901535034 | Learning Rate: 1.0000000000000009e-15\n",
      "Step 14501 | Loss: 2.441843032836914 | Learning Rate: 1.0000000000000009e-15\n",
      "Step 14601 | Loss: 2.4889674186706543 | Learning Rate: 1.0000000000000009e-15\n",
      "Step 14701 | Loss: 2.250256061553955 | Learning Rate: 1.0000000000000009e-15\n",
      "Step 14801 | Loss: 2.2369191646575928 | Learning Rate: 1.0000000000000009e-15\n",
      "Step 14901 | Loss: 2.4150230884552 | Learning Rate: 1.0000000000000009e-15\n",
      "Step 15001 | Loss: 2.423652172088623 | Learning Rate: 1.0000000000000009e-15\n",
      "Step 15101 | Loss: 2.259870767593384 | Learning Rate: 1.000000000000001e-16\n",
      "Step 15201 | Loss: 2.6079328060150146 | Learning Rate: 1.000000000000001e-16\n",
      "Step 15301 | Loss: 1.789175033569336 | Learning Rate: 1.000000000000001e-16\n",
      "Step 15401 | Loss: 2.290432929992676 | Learning Rate: 1.000000000000001e-16\n",
      "Step 15501 | Loss: 2.3037283420562744 | Learning Rate: 1.000000000000001e-16\n",
      "Step 15601 | Loss: 2.1096787452697754 | Learning Rate: 1.000000000000001e-16\n",
      "Step 15701 | Loss: 2.3983280658721924 | Learning Rate: 1.000000000000001e-16\n",
      "Step 15801 | Loss: 2.8391287326812744 | Learning Rate: 1.000000000000001e-16\n",
      "Step 15901 | Loss: 2.368319272994995 | Learning Rate: 1.000000000000001e-16\n",
      "Step 16001 | Loss: 2.104422092437744 | Learning Rate: 1.000000000000001e-16\n",
      "Step 16101 | Loss: 2.3051044940948486 | Learning Rate: 1.000000000000001e-17\n",
      "Step 16201 | Loss: 2.3505215644836426 | Learning Rate: 1.000000000000001e-17\n",
      "Step 16301 | Loss: 2.357905387878418 | Learning Rate: 1.000000000000001e-17\n",
      "Step 16401 | Loss: 2.228837013244629 | Learning Rate: 1.000000000000001e-17\n",
      "Step 16501 | Loss: 2.249293327331543 | Learning Rate: 1.000000000000001e-17\n",
      "Step 16601 | Loss: 2.298600673675537 | Learning Rate: 1.000000000000001e-17\n",
      "Step 16701 | Loss: 2.575855255126953 | Learning Rate: 1.000000000000001e-17\n",
      "Step 16801 | Loss: 2.3296542167663574 | Learning Rate: 1.000000000000001e-17\n",
      "Step 16901 | Loss: 2.3620452880859375 | Learning Rate: 1.000000000000001e-17\n",
      "Step 17001 | Loss: 2.5600788593292236 | Learning Rate: 1.000000000000001e-17\n",
      "Step 17101 | Loss: 2.2349436283111572 | Learning Rate: 1.000000000000001e-18\n",
      "Step 17201 | Loss: 2.425363302230835 | Learning Rate: 1.000000000000001e-18\n",
      "Step 17301 | Loss: 2.2100017070770264 | Learning Rate: 1.000000000000001e-18\n",
      "Step 17401 | Loss: 2.1906280517578125 | Learning Rate: 1.000000000000001e-18\n",
      "Step 17501 | Loss: 2.4010043144226074 | Learning Rate: 1.000000000000001e-18\n",
      "Step 17601 | Loss: 2.563286066055298 | Learning Rate: 1.000000000000001e-18\n",
      "Step 17701 | Loss: 1.9713354110717773 | Learning Rate: 1.000000000000001e-18\n",
      "Step 17801 | Loss: 2.298109531402588 | Learning Rate: 1.000000000000001e-18\n",
      "Step 17901 | Loss: 2.1165730953216553 | Learning Rate: 1.000000000000001e-18\n",
      "Step 18001 | Loss: 2.218602180480957 | Learning Rate: 1.000000000000001e-18\n",
      "Step 18101 | Loss: 2.184152364730835 | Learning Rate: 1.000000000000001e-19\n",
      "Step 18201 | Loss: 2.393416166305542 | Learning Rate: 1.000000000000001e-19\n",
      "Step 18301 | Loss: 2.499823808670044 | Learning Rate: 1.000000000000001e-19\n",
      "Step 18401 | Loss: 2.4552230834960938 | Learning Rate: 1.000000000000001e-19\n",
      "Step 18501 | Loss: 2.1318094730377197 | Learning Rate: 1.000000000000001e-19\n",
      "Step 18601 | Loss: 2.2623493671417236 | Learning Rate: 1.000000000000001e-19\n",
      "Step 18701 | Loss: 2.4273781776428223 | Learning Rate: 1.000000000000001e-19\n",
      "Step 18801 | Loss: 2.3818657398223877 | Learning Rate: 1.000000000000001e-19\n",
      "Step 18901 | Loss: 2.2343311309814453 | Learning Rate: 1.000000000000001e-19\n",
      "Step 19001 | Loss: 2.3791260719299316 | Learning Rate: 1.000000000000001e-19\n",
      "Step 19101 | Loss: 2.2252135276794434 | Learning Rate: 1.0000000000000011e-20\n",
      "Step 19201 | Loss: 2.279282808303833 | Learning Rate: 1.0000000000000011e-20\n",
      "Step 19301 | Loss: 2.408745288848877 | Learning Rate: 1.0000000000000011e-20\n",
      "Step 19401 | Loss: 2.506183624267578 | Learning Rate: 1.0000000000000011e-20\n",
      "Step 19501 | Loss: 2.2342958450317383 | Learning Rate: 1.0000000000000011e-20\n",
      "Step 19601 | Loss: 2.2958879470825195 | Learning Rate: 1.0000000000000011e-20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 148\u001b[0m\n\u001b[1;32m    145\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# update parameters\u001b[39;00m\n\u001b[1;32m    151\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tinygrad/tensor.py:768\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t0 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deepwalk()):\n\u001b[1;32m    767\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m t0\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 768\u001b[0m   token \u001b[38;5;241m=\u001b[39m _METADATA\u001b[38;5;241m.\u001b[39mset(\u001b[43mdataclasses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m (md \u001b[38;5;241m:=\u001b[39m t0\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mmetadata) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    769\u001b[0m   grads \u001b[38;5;241m=\u001b[39m t0\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mbackward(t0\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mlazydata)\n\u001b[1;32m    770\u001b[0m   _METADATA\u001b[38;5;241m.\u001b[39mreset(token)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tinygrad.helpers import Timing\n",
    "from tinygrad import Tensor\n",
    "from tinygrad import dtypes\n",
    "from tinygrad.nn.optim import SGD\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "def sparse_categorical_crossentropy(self, Y, ignore_index=-1) -> Tensor:\n",
    "    loss_mask = Y != ignore_index\n",
    "    y_counter = Tensor.arange(self.shape[-1], dtype=dtypes.int32, requires_grad=False, device=self.device).unsqueeze(0).expand(Y.numel(), self.shape[-1])\n",
    "    y = ((y_counter == Y.flatten().reshape(-1, 1)).where(-1.0, 0) * loss_mask.reshape(-1, 1)).reshape(*Y.shape, self.shape[-1])\n",
    "    return self.log_softmax().mul(y).sum() / loss_mask.sum()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias=True, initialization: str='kaiming_uniform'):\n",
    "        self.weight = getattr(Tensor, initialization)(in_features, out_features)\n",
    "        self.bias = Tensor.zeros(out_features) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x.linear(self.weight.transpose(), self.bias)\n",
    "\n",
    "class TinyNet:\n",
    "    def __init__(self):\n",
    "        self.C = Linear(27, 10, bias=False)\n",
    "        self.W1 = Linear(30, 200, bias=True)\n",
    "        self.W2 = Linear(200, 27, bias=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        emb = self.C.weight[x]\n",
    "        h = Tensor.tanh(emb.view(-1, 30) @ self.W1.weight + self.W1.bias)\n",
    "        logits = h @ self.W2.weight + self.W2.bias\n",
    "        loss = sparse_categorical_crossentropy(logits, Ytr[ix])\n",
    "        return loss\n",
    "\n",
    "    def sample(self, itos, block_size=3, num_samples=5):\n",
    "        \"\"\"\n",
    "        Generate samples from the trained model.\n",
    "\n",
    "        Args:\n",
    "            itos (dict): Dictionary mapping indices to characters.\n",
    "            block_size (int): Number of previous characters to use as context.\n",
    "            num_samples (int): Number of samples to generate.\n",
    "        \"\"\"\n",
    "        for _ in range(num_samples):\n",
    "            output = []\n",
    "            context = [0] * block_size  # Initialize with the start token index\n",
    "            while True:\n",
    "                # Convert context to a Tensor\n",
    "                context_tensor = Tensor(np.array([context]), dtype=dtypes.int32, requires_grad=False, device=self.C.weight.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                emb = self.C.weight[context_tensor]  # Shape: (1, block_size, out_features)\n",
    "                emb = emb.view(1, -1)  # Flatten the embedding\n",
    "                h = Tensor.tanh(emb @ self.W1.weight + self.W1.bias)  # Hidden layer\n",
    "                logits = h @ self.W2.weight + self.W2.bias  # Output logits\n",
    "                \n",
    "                # Apply softmax to get probabilities\n",
    "                probs = Tensor.softmax(logits).numpy().flatten()\n",
    "                \n",
    "                # Handle potential numerical issues by normalizing\n",
    "                probs = probs / probs.sum()\n",
    "                \n",
    "                # Sample from the probability distribution\n",
    "                ix = np.random.choice(len(probs), p=probs)\n",
    "                \n",
    "                # Update context and output\n",
    "                context = context[1:] + [ix]\n",
    "                output.append(ix)\n",
    "                \n",
    "                # End token encountered\n",
    "                if ix == 0:\n",
    "                    break\n",
    "            \n",
    "            # Convert indices to characters and print the generated word\n",
    "            generated_word = ''.join(itos[i] for i in output)\n",
    "            print(generated_word)\n",
    "\n",
    "net = TinyNet()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "words = open('./sandbox/names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3  # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "      \n",
    "X = Tensor(X)\n",
    "Y = Tensor(Y)\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3  # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] \n",
    "    X = Tensor(X)\n",
    "    Y = Tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.1\n",
    "opt = SGD([net.C.weight, net.W1.weight, net.W1.bias, net.W2.weight, net.W2.bias], lr=learning_rate)\n",
    "\n",
    "with Tensor.train():\n",
    "    for step in range(200000):\n",
    "        # random sample a batch / Minibatch construct \n",
    "        ix = Tensor.randint(32, low=0, high=Xtr.shape[0])\n",
    "\n",
    "        # forward pass / Forward pass\n",
    "        loss = net(Xtr[ix])\n",
    "        \n",
    "        # zero gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        opt.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step+1} | Loss: {loss.numpy()} | Learning Rate: {learning_rate}\")\n",
    "        \n",
    "        if step > 100 and step % 1000 == 0:\n",
    "            learning_rate *= 0.1\n",
    "            opt.lr = learning_rate  # Update optimizer's learning rate\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "# Generate and print samples after training\n",
    "net.sample(itos, block_size=block_size, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69c86e09-269e-44e6-b8ea-1021de25a320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rum.\n",
      "miriya.\n",
      "dricket.\n",
      "horaty.\n",
      "bic.\n"
     ]
    }
   ],
   "source": [
    "net.sample(itos, block_size=block_size, num_samples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
